# -*- coding: utf-8 -*-
"""Text-Preprocessing-NLP .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16sbYHvGy5M8Ahc7Eu9Sv00aTBytzksFl

About Dataset:
The dataset contains a total of 31,613 paragraphs that are separated by new lines. The content of each paragraph is not specified, but it can be assumed that the paragraphs cover a wide range of topics and subjects.

Step 1: Load the .txt File
"""

# Read the .txt file
file_path = '/content/paragraphs.txt'  # Replace with your actual file path
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# Display the first few characters to verify
print(text[:500])  # Display the first 500 characters

"""Step 2: Text Cleaning"""

import string
import re

#Converting text to lowercase
text = text.lower()

#Removing puctuation
# re.sub(pattern, replacement, text)
text = re.sub(f'[{string.punctuation}]', '', text)

#Removing digits
text = re.sub(r'\d+', '', text)

#Removing extra white spaces
text = re.sub(r'\s+', '', text)

"""Step 3: Tokenization"""

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize

#Split text into sentences
sentences = sent_tokenize(text)

#Split text into words
words = word_tokenize(text)

print(sentences[:5])
print(words[:5])

"""Step 4: Stopwords Removal"""

from nltk.corpus import stopwords

#Downloading stopwords
nltk.download("stopwords")

#Get the list of stop_words
stop_words = set(stopwords.words("english"))

#Removes topwards
cleaned_words = [word for word in words if word not in stop_words]

print(words[:10])
print(cleaned_words[:10])

"""Step 5: Stemming/Lemmatization

Stemming: Stemming is a rule-based process of removing suffixes from a word to get its base or root form, which may not always be a proper word. It operates purely on a set of rules without understanding the context or grammar.

Lemmatization: Lemmatization is a more sophisticated process that reduces words to their base or dictionary form (lemma), considering the wordâ€™s context and part of speech (e.g., noun, verb, adjective).

Example Comparison
For the word "better":

Stemming: "bett" (not meaningful)

Lemmatization: "good" (correct form considering context)

"""

from nltk.stem import PorterStemmer, WordNetLemmatizer

# Stemming example
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

stemmed_words = [stemmer.stem(word) for word in cleaned_words]

# Lemmatization example
nltk.download('wordnet')
lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_words]
print(cleaned_words[:10])
print(lemmatized_words[:10])

print(cleaned_words[:50])
print(lemmatized_words[:50])